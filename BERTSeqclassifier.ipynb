{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7RulW8_ZugHt"},"outputs":[],"source":["import torch\n","print(torch.cuda.memory_allocated())\n","print(torch.cuda.memory_reserved())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73xtUqwxugHv"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6hzRaHVugHw"},"outputs":[],"source":["df = pd.read_csv('jigsaw_toxicity_processed3.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PICcKlCpugHw"},"outputs":[],"source":["df['labels'] = df.loc[:, ['score','non-toxic']].values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfh0jFgKugHx"},"outputs":[],"source":["df.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjrM3M3ZugHx"},"outputs":[],"source":["import datasets\n","dataset = datasets.Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNCa5fKRugHy"},"outputs":[],"source":["print(len(dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXIUZO9iugHy"},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lfz_KG8wugHz"},"outputs":[],"source":["#from transformers import BertTokenizer\n","from transformers import BertTokenizer\n","\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","dataset = dataset.map(\n","    lambda x: tokenizer(\n","            x['comment_text'], max_length=256, padding='max_length',\n","            truncation=True\n","        ), batched=True\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YiaaEz6ugH0"},"outputs":[],"source":["dataset = dataset.remove_columns(['comment_text'])\n","dataset = dataset.remove_columns(['Unnamed: 0'])\n","dataset = dataset.remove_columns(['Unnamed: 0.1'])\n","dataset.set_format(type='torch', output_all_columns=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCONcxJvugH0"},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKy3nCk8ugH1"},"outputs":[],"source":["import torch\n","batch_size = 16\n","loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WtkML7sugH1"},"outputs":[],"source":["from transformers import BertModel , BertForSequenceClassification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKo9ySBkugH1"},"outputs":[],"source":["# # Get all of the model's parameters as a list of tuples.\n","# params = list(model.named_parameters())\n","\n","# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","# print('==== Embedding Layer ====\\n')\n","\n","# for p in params[0:5]:\n","#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","# print('\\n==== First Transformer ====\\n')\n","\n","# for p in params[5:21]:\n","#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","# print('\\n==== Output Layer ====\\n')\n","\n","# for p in params[-4:]:\n","#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3otFKVQSugH2"},"outputs":[],"source":["# # define mean pooling function\n","# def mean_pool(token_embeds, attention_mask):\n","#     # reshape attention_mask to cover 768-dimension embeddings\n","#     in_mask = attention_mask.unsqueeze(-1).expand(\n","#         token_embeds.size()\n","#     ).float()\n","#     # perform mean-pooling but exclude padding tokens (specified by in_mask)\n","#     pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n","#         in_mask.sum(1), min=1e-9\n","#     )\n","#     return pool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqgLj-k1ugH2"},"outputs":[],"source":["#loss_func = torch.nn.BCEWithLogitsLoss()#CrossEntropyLoss()\n","#linear = torch.nn.Linear(768,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTh0QKwRugH2"},"outputs":[],"source":["def loss_fn(outputs, targets):  # set the loss_fn \n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"si4XVSoGugH2"},"outputs":[],"source":["#set device and move model there\n","device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","#loss_fn.to(device)\n","#linear.to(device)\n","print(f'moved to {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vP_W2RpmugH3"},"outputs":[],"source":["#print(torch.cuda.memory_allocated())\n","#print(torch.cuda.memory_reserved())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tofcwbfHugH3"},"outputs":[],"source":["from transformers.optimization import get_linear_schedule_with_warmup\n","\n","# initialize Adam optimizer\n","optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n","\n","# setup warmup for first ~10% of steps\n","total_steps = int(len(dataset['input_ids']) / batch_size)\n","warmup_steps = int(0.1 * total_steps)\n","scheduler = get_linear_schedule_with_warmup(\n","    optim, num_warmup_steps=warmup_steps,\n","    num_training_steps=total_steps-warmup_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z26i8oRmugH3"},"outputs":[],"source":["all_loss=[]\n","all_batch=[]\n","all_epoch=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KX335eWlugH3"},"outputs":[],"source":["from tqdm.auto import tqdm\n","\n","epochs = 2\n","# 1 epoch should be enough, increase if wanted\n","for epoch in range(epochs):\n","    model.train()  # make sure model is in training mode\n","    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n","    loop = tqdm(loader,leave=True)\n","    #for batch in loop:\n","    for _,batch in enumerate(loop):\n","        # zero all gradients on each new step\n","        optim.zero_grad()\n","        \n","        # prepare batches and more all to the active device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = loss_fn(outputs.logits, labels)\n","        loss.backward()\n","        optim.step()\n","        scheduler.step()\n","        \n","        # update the TDQM progress bar\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())\n","        loss = loss.item()\n","        all_loss.append(loss)\n","        all_batch.append(_)\n","        all_epoch.append(epoch)\n","        if _%10==0:\n","            print(f'Epoch {str(epoch)} Btach {str(_)} Loss {str(loss)}')\n","            \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cP5EbfsugH4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","l=[]\n","l.extend(range(len(all_loss)))\n","plt.plot(l,all_loss, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nW43tSCsugH4"},"outputs":[],"source":["import os\n","\n","model_path = './model_save3'\n","\n","if not os.path.exists(model_path):\n","    os.mkdir(model_path)\n","\n","model.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6BoKehkugH4"},"outputs":[],"source":["all_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PilkK2YugH4"},"outputs":[],"source":["loss_dict = {'loss': all_loss}        \n","loss_df = pd.DataFrame(loss_dict) \n","loss_df.to_csv('loss.csv') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvgfWCWDugH5"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"BERTSeqclassifier.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}